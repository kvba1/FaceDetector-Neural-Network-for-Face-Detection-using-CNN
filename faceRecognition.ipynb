{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Resize\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.io import read_image\n",
    "import json\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import albumentations as alb\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "from torch import nn\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentor = alb.Compose([alb.RandomCrop(width=800, height=800), \n",
    "                         alb.HorizontalFlip(p=0.5), \n",
    "                         alb.RandomBrightnessContrast(p=0.2),\n",
    "                         alb.RandomGamma(p=0.2), \n",
    "                         alb.RGBShift(p=0.2), \n",
    "                         alb.VerticalFlip(p=0.5)], \n",
    "                       bbox_params=alb.BboxParams(format='albumentations', \n",
    "                                                  label_fields=['class_labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in ['train','test']:\n",
    "    for file in os.listdir(os.path.join('data', folder, 'images')):\n",
    "        \n",
    "        filename = file.split('.')[0]+'.json'\n",
    "        existing_filepath = os.path.join('data','labels', filename)\n",
    "        if os.path.exists(existing_filepath): \n",
    "            new_filepath = os.path.join('data',folder,'labels',filename)\n",
    "            os.replace(existing_filepath, new_filepath)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image must be numpy array type\n",
      "image must be numpy array type\n"
     ]
    }
   ],
   "source": [
    "for partition in ['train','test']: \n",
    "    for image in os.listdir(os.path.join('data', partition, 'images')):\n",
    "        img = cv2.imread(os.path.join('data', partition, 'images', image))\n",
    "\n",
    "        coords = [0,0,0.00001,0.00001]\n",
    "        label_path = os.path.join('data', partition, 'labels', f'{image.split(\".\")[0]}.json')\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as f:\n",
    "                label = json.load(f)\n",
    "\n",
    "            coords[0] = label['shapes'][0]['points'][0][0]\n",
    "            coords[1] = label['shapes'][0]['points'][0][1]\n",
    "            coords[2] = label['shapes'][0]['points'][1][0]\n",
    "            coords[3] = label['shapes'][0]['points'][1][1]\n",
    "            coords = list(np.divide(coords, [1920,1080,1920,1080]))\n",
    "\n",
    "        try: \n",
    "            for x in range(80):\n",
    "                augmented = augmentor(image=img, bboxes=[coords], class_labels=['face'])\n",
    "                image_name = f'{image.split(\".\")[0]}.{x}.jpg'\n",
    "                cv2.imwrite(os.path.join('aug_data', partition, 'images', image_name),  augmented['image'])\n",
    "\n",
    "                annotation = {}\n",
    "                annotation['image'] = image_name\n",
    "\n",
    "                if os.path.exists(label_path):\n",
    "                    if len(augmented['bboxes']) == 0: \n",
    "                        annotation['bbox'] = [0,0,0,0]\n",
    "                        annotation['class'] = 0 \n",
    "                    else: \n",
    "                        annotation['bbox'] = augmented['bboxes'][0]\n",
    "                        annotation['class'] = 1\n",
    "                else: \n",
    "                    annotation['bbox'] = [0,0,0,0]\n",
    "                    annotation['class'] = 0 \n",
    "\n",
    "\n",
    "                with open(os.path.join('aug_data', partition, 'labels', f'{image.split(\".\")[0]}.{x}.json'), 'w') as f:\n",
    "                    json.dump(annotation, f)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDataset(Dataset):\n",
    "    def __init__(self, labels_dir, images_dir, transform=None):\n",
    "        \n",
    "        self.images_dir = images_dir\n",
    "        self.transform = transform\n",
    "        self.annotations = []\n",
    "\n",
    "        for filename in os.listdir(labels_dir):\n",
    "            if filename.endswith('.json'):\n",
    "                json_path = os.path.join(labels_dir, filename)\n",
    "                with open(json_path) as f:\n",
    "                    data = json.load(f)\n",
    "                    self.annotations.append(data)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_info = self.annotations[idx]\n",
    "        \n",
    "        image = Image.open(os.path.join(self.images_dir, img_info['image'])).convert('RGB')\n",
    "        bbox = img_info['bbox']\n",
    "        boxes = torch.Tensor([bbox[0], bbox[1], bbox[2], bbox[3]])\n",
    "        classes = img_info['class']\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, (classes, boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    Resize((240,240)),\n",
    "    ToTensor()\n",
    "    \n",
    "])\n",
    "\n",
    "training_data = FaceDataset(labels_dir='aug_data/train/labels',\n",
    "                           images_dir='aug_data/train/images',\n",
    "                           transform=transform)\n",
    "testing_data = FaceDataset(labels_dir='aug_data/test/labels',\n",
    "                           images_dir='aug_data/test/images',\n",
    "                           transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(testing_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([64, 3, 240, 240])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.4431, 0.4471, 0.5137,  ..., 0.4941, 0.4902, 0.4863],\n",
       "          [0.4471, 0.4510, 0.5176,  ..., 0.4980, 0.4941, 0.4902],\n",
       "          [0.4549, 0.4549, 0.5255,  ..., 0.4980, 0.4941, 0.4902],\n",
       "          ...,\n",
       "          [0.6549, 0.6549, 0.6549,  ..., 0.5255, 0.5255, 0.5216],\n",
       "          [0.6549, 0.6549, 0.6549,  ..., 0.5255, 0.5255, 0.5216],\n",
       "          [0.6510, 0.6510, 0.6510,  ..., 0.5255, 0.5216, 0.5176]],\n",
       " \n",
       "         [[0.3804, 0.3843, 0.4471,  ..., 0.5176, 0.5137, 0.5176],\n",
       "          [0.3843, 0.3882, 0.4549,  ..., 0.5216, 0.5176, 0.5216],\n",
       "          [0.3922, 0.3922, 0.4588,  ..., 0.5255, 0.5216, 0.5176],\n",
       "          ...,\n",
       "          [0.6784, 0.6784, 0.6784,  ..., 0.5373, 0.5333, 0.5294],\n",
       "          [0.6784, 0.6784, 0.6784,  ..., 0.5333, 0.5333, 0.5294],\n",
       "          [0.6745, 0.6745, 0.6745,  ..., 0.5333, 0.5294, 0.5255]],\n",
       " \n",
       "         [[0.2824, 0.2863, 0.3647,  ..., 0.2353, 0.2314, 0.2314],\n",
       "          [0.2863, 0.2980, 0.3686,  ..., 0.2392, 0.2353, 0.2353],\n",
       "          [0.2980, 0.3059, 0.3765,  ..., 0.2431, 0.2392, 0.2353],\n",
       "          ...,\n",
       "          [0.3255, 0.3255, 0.3255,  ..., 0.2353, 0.2392, 0.2353],\n",
       "          [0.3255, 0.3255, 0.3255,  ..., 0.2392, 0.2392, 0.2353],\n",
       "          [0.3216, 0.3216, 0.3216,  ..., 0.2392, 0.2353, 0.2353]]]),\n",
       " tensor([0.1670, 0.3243, 0.5568, 0.9460]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features, (classes, bboxes) = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "img = train_features[2].squeeze()\n",
    "label = bboxes[2]\n",
    "\n",
    "img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/torch/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/torch/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Conv2d: 1-1                            [-1, 64, 240, 240]        (1,792)\n",
      "├─ReLU: 1-2                              [-1, 64, 240, 240]        --\n",
      "├─Conv2d: 1-3                            [-1, 64, 240, 240]        (36,928)\n",
      "├─ReLU: 1-4                              [-1, 64, 240, 240]        --\n",
      "├─MaxPool2d: 1-5                         [-1, 64, 120, 120]        --\n",
      "├─Conv2d: 1-6                            [-1, 128, 120, 120]       (73,856)\n",
      "├─ReLU: 1-7                              [-1, 128, 120, 120]       --\n",
      "├─Conv2d: 1-8                            [-1, 128, 120, 120]       (147,584)\n",
      "├─ReLU: 1-9                              [-1, 128, 120, 120]       --\n",
      "├─MaxPool2d: 1-10                        [-1, 128, 60, 60]         --\n",
      "├─Conv2d: 1-11                           [-1, 256, 60, 60]         (295,168)\n",
      "├─ReLU: 1-12                             [-1, 256, 60, 60]         --\n",
      "├─Conv2d: 1-13                           [-1, 256, 60, 60]         (590,080)\n",
      "├─ReLU: 1-14                             [-1, 256, 60, 60]         --\n",
      "├─Conv2d: 1-15                           [-1, 256, 60, 60]         (590,080)\n",
      "├─ReLU: 1-16                             [-1, 256, 60, 60]         --\n",
      "├─MaxPool2d: 1-17                        [-1, 256, 30, 30]         --\n",
      "├─Conv2d: 1-18                           [-1, 512, 30, 30]         (1,180,160)\n",
      "├─ReLU: 1-19                             [-1, 512, 30, 30]         --\n",
      "├─Conv2d: 1-20                           [-1, 512, 30, 30]         (2,359,808)\n",
      "├─ReLU: 1-21                             [-1, 512, 30, 30]         --\n",
      "├─Conv2d: 1-22                           [-1, 512, 30, 30]         (2,359,808)\n",
      "├─ReLU: 1-23                             [-1, 512, 30, 30]         --\n",
      "├─MaxPool2d: 1-24                        [-1, 512, 15, 15]         --\n",
      "├─Conv2d: 1-25                           [-1, 512, 15, 15]         (2,359,808)\n",
      "├─ReLU: 1-26                             [-1, 512, 15, 15]         --\n",
      "├─Conv2d: 1-27                           [-1, 512, 15, 15]         (2,359,808)\n",
      "├─ReLU: 1-28                             [-1, 512, 15, 15]         --\n",
      "├─Conv2d: 1-29                           [-1, 512, 15, 15]         (2,359,808)\n",
      "├─ReLU: 1-30                             [-1, 512, 15, 15]         --\n",
      "├─MaxPool2d: 1-31                        [-1, 512, 7, 7]           --\n",
      "==========================================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "Total mult-adds (G): 17.62\n",
      "==========================================================================================\n",
      "Input size (MB): 0.66\n",
      "Forward/backward pass size (MB): 118.65\n",
      "Params size (MB): 56.13\n",
      "Estimated Total Size (MB): 175.44\n",
      "==========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "├─Conv2d: 1-1                            [-1, 64, 240, 240]        (1,792)\n",
       "├─ReLU: 1-2                              [-1, 64, 240, 240]        --\n",
       "├─Conv2d: 1-3                            [-1, 64, 240, 240]        (36,928)\n",
       "├─ReLU: 1-4                              [-1, 64, 240, 240]        --\n",
       "├─MaxPool2d: 1-5                         [-1, 64, 120, 120]        --\n",
       "├─Conv2d: 1-6                            [-1, 128, 120, 120]       (73,856)\n",
       "├─ReLU: 1-7                              [-1, 128, 120, 120]       --\n",
       "├─Conv2d: 1-8                            [-1, 128, 120, 120]       (147,584)\n",
       "├─ReLU: 1-9                              [-1, 128, 120, 120]       --\n",
       "├─MaxPool2d: 1-10                        [-1, 128, 60, 60]         --\n",
       "├─Conv2d: 1-11                           [-1, 256, 60, 60]         (295,168)\n",
       "├─ReLU: 1-12                             [-1, 256, 60, 60]         --\n",
       "├─Conv2d: 1-13                           [-1, 256, 60, 60]         (590,080)\n",
       "├─ReLU: 1-14                             [-1, 256, 60, 60]         --\n",
       "├─Conv2d: 1-15                           [-1, 256, 60, 60]         (590,080)\n",
       "├─ReLU: 1-16                             [-1, 256, 60, 60]         --\n",
       "├─MaxPool2d: 1-17                        [-1, 256, 30, 30]         --\n",
       "├─Conv2d: 1-18                           [-1, 512, 30, 30]         (1,180,160)\n",
       "├─ReLU: 1-19                             [-1, 512, 30, 30]         --\n",
       "├─Conv2d: 1-20                           [-1, 512, 30, 30]         (2,359,808)\n",
       "├─ReLU: 1-21                             [-1, 512, 30, 30]         --\n",
       "├─Conv2d: 1-22                           [-1, 512, 30, 30]         (2,359,808)\n",
       "├─ReLU: 1-23                             [-1, 512, 30, 30]         --\n",
       "├─MaxPool2d: 1-24                        [-1, 512, 15, 15]         --\n",
       "├─Conv2d: 1-25                           [-1, 512, 15, 15]         (2,359,808)\n",
       "├─ReLU: 1-26                             [-1, 512, 15, 15]         --\n",
       "├─Conv2d: 1-27                           [-1, 512, 15, 15]         (2,359,808)\n",
       "├─ReLU: 1-28                             [-1, 512, 15, 15]         --\n",
       "├─Conv2d: 1-29                           [-1, 512, 15, 15]         (2,359,808)\n",
       "├─ReLU: 1-30                             [-1, 512, 15, 15]         --\n",
       "├─MaxPool2d: 1-31                        [-1, 512, 7, 7]           --\n",
       "==========================================================================================\n",
       "Total params: 14,714,688\n",
       "Trainable params: 0\n",
       "Non-trainable params: 14,714,688\n",
       "Total mult-adds (G): 17.62\n",
       "==========================================================================================\n",
       "Input size (MB): 0.66\n",
       "Forward/backward pass size (MB): 118.65\n",
       "Params size (MB): 56.13\n",
       "Estimated Total Size (MB): 175.44\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16 = models.vgg16(pretrained=True)\n",
    "model = vgg16.features\n",
    "\n",
    "for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "summary(model, (3, 240, 240))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceTracker(nn.Module):\n",
    "    def __init__(self, vgg_model):\n",
    "        super(FaceTracker, self).__init__()\n",
    "        self.features = vgg_model\n",
    "\n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512 * 7 * 7, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(2048, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Bounding box regression head\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512 * 7 * 7, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(2048, 4),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        class_output = self.classifier(x)\n",
    "        bbox_output = self.regressor(x)\n",
    "        return class_output, bbox_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "FaceTrackerModel = FaceTracker(model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_class = nn.BCELoss()\n",
    "criterion_bbox = nn.SmoothL1Loss()\n",
    "optimizer = Adam(FaceTrackerModel.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion_class, criterion_bbox, optimizer, epochs):\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        total_class_loss = 0.0\n",
    "        total_bbox_loss = 0.0\n",
    "        \n",
    "        for images, (classes, bboxes) in train_loader:\n",
    "            images = images.to(device)\n",
    "            classes = classes.to(device)\n",
    "            bboxes = bboxes.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            class_preds, bbox_preds = model(images)\n",
    "\n",
    "            loss_class = criterion_class(class_preds.squeeze(), classes.float())\n",
    "            loss_bbox = criterion_bbox(bbox_preds, bboxes)\n",
    "            loss = loss_class + loss_bbox\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_class_loss += loss_class.item()\n",
    "            total_bbox_loss += loss_bbox.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        avg_class_loss = total_class_loss / len(train_loader)\n",
    "        avg_bbox_loss = total_bbox_loss / len(train_loader)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}: Total Loss: {avg_loss:.4f}, Class Loss: {avg_class_loss:.4f}, BBox Loss: {avg_bbox_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, criterion_class, criterion_bbox, device):\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_class_loss = 0.0\n",
    "    total_bbox_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, (classes, bboxes) in test_loader:\n",
    "            images = images.to(device)\n",
    "            classes = classes.to(device)\n",
    "            bboxes = bboxes.to(device)\n",
    "\n",
    "            class_preds, bbox_preds = model(images)\n",
    "\n",
    "            loss_class = criterion_class(class_preds.squeeze(), classes.float())\n",
    "            loss_bbox = criterion_bbox(bbox_preds, bboxes)\n",
    "            loss = loss_class + loss_bbox\n",
    "\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            total_class_loss += loss_class.item() * images.size(0)\n",
    "            total_bbox_loss += loss_bbox.item() * images.size(0)\n",
    "            total_samples += images.size(0)\n",
    "\n",
    "        avg_loss = total_loss / total_samples\n",
    "        avg_class_loss = total_class_loss / total_samples\n",
    "        avg_bbox_loss = total_bbox_loss / total_samples\n",
    "\n",
    "    print(f'Test Results - Total Loss: {avg_loss:.4f}, Class Loss: {avg_class_loss:.4f}, BBox Loss: {avg_bbox_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Total Loss: 0.2249, Class Loss: 0.2119, BBox Loss: 0.0130\n",
      "Epoch 2: Total Loss: 0.1153, Class Loss: 0.1082, BBox Loss: 0.0070\n",
      "Epoch 3: Total Loss: 0.0757, Class Loss: 0.0705, BBox Loss: 0.0052\n",
      "Epoch 4: Total Loss: 0.0531, Class Loss: 0.0490, BBox Loss: 0.0041\n",
      "Epoch 5: Total Loss: 0.0403, Class Loss: 0.0371, BBox Loss: 0.0033\n",
      "Epoch 6: Total Loss: 0.0304, Class Loss: 0.0275, BBox Loss: 0.0028\n",
      "Epoch 7: Total Loss: 0.0215, Class Loss: 0.0193, BBox Loss: 0.0022\n",
      "Epoch 8: Total Loss: 0.0171, Class Loss: 0.0152, BBox Loss: 0.0019\n",
      "Epoch 9: Total Loss: 0.0144, Class Loss: 0.0129, BBox Loss: 0.0016\n",
      "Epoch 10: Total Loss: 0.0113, Class Loss: 0.0099, BBox Loss: 0.0014\n"
     ]
    }
   ],
   "source": [
    "FaceTrackerModel.train()\n",
    "train_loader = train_dataloader\n",
    "\n",
    "for epoch in range(10):\n",
    "    total_loss = 0.0\n",
    "    total_class_loss = 0.0\n",
    "    total_bbox_loss = 0.0\n",
    "        \n",
    "    for images, (classes, bboxes) in train_loader:\n",
    "        images = images.to(device)\n",
    "        classes = classes.to(device)\n",
    "        bboxes = bboxes.to(device)\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "        class_preds, bbox_preds = FaceTrackerModel(images)\n",
    "            \n",
    "        loss_class = criterion_class(class_preds.squeeze(), classes.float())\n",
    "        loss_bbox = criterion_bbox(bbox_preds, bboxes)\n",
    "        \n",
    "        loss = loss_class + loss_bbox\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "        total_loss += loss.item()\n",
    "        total_class_loss += loss_class.item()\n",
    "        total_bbox_loss += loss_bbox.item()\n",
    "        \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    avg_class_loss = total_class_loss / len(train_loader)\n",
    "    avg_bbox_loss = total_bbox_loss / len(train_loader)\n",
    "        \n",
    "    print(f'Epoch {epoch+1}: Total Loss: {avg_loss:.4f}, Class Loss: {avg_class_loss:.4f}, BBox Loss: {avg_bbox_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(FaceTrackerModel.state_dict(), './models/test2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FaceTrackerModel.load_state_dict(torch.load('./models/test2.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results - Total Loss: 0.0796, Class Loss: 0.0735, BBox Loss: 0.0061\n"
     ]
    }
   ],
   "source": [
    "test(FaceTrackerModel, test_dataloader, criterion_class, criterion_bbox, device='mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=4, figsize=(20, 20))\n",
    "FaceTrackerModel.eval()\n",
    "\n",
    "sample_images, _ = next(iter(train_dataloader))\n",
    "\n",
    "for idx in range(4): \n",
    "    sample_image = sample_images[idx]\n",
    "    mps_img = sample_image.to('mps').unsqueeze(0)\n",
    "    \n",
    "    class_pred, bbox_pred = FaceTrackerModel(mps_img)\n",
    "    \n",
    "    bbox_pred = bbox_pred.squeeze(0).cpu().detach().numpy()\n",
    "        \n",
    "    start_point = tuple((bbox_pred[:2] * [240, 240]).astype(int))\n",
    "    end_point = tuple((bbox_pred[2:] * [240, 240]).astype(int))\n",
    "        \n",
    "    sample_image_np = sample_image.permute(1, 2, 0).numpy()\n",
    "    sample_image_np = (sample_image_np * 255).astype(np.uint8)\n",
    "    sample_image_np = np.ascontiguousarray(sample_image_np)\n",
    "\n",
    "    cv2.rectangle(sample_image_np, start_point, end_point, (255, 0, 0), 2)\n",
    "        \n",
    "    ax[idx].imshow(sample_image_np)\n",
    "    ax[idx].axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
